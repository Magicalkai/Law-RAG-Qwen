import torch
import pandas as pd
import numpy as np
import faiss
from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM
from peft import PeftModel
import torch.nn.functional as F
from sentence_transformers import SentenceTransformer # ä½¿ç”¨ sentence_transformers åº“ç®€åŒ– BGE åŠ è½½

# =========================== é…ç½®åŒºåŸŸ ===========================
# æ£€ç´¢æ¨¡å‹ï¼šBGE-Small (è‡ªåŠ¨ä¸‹è½½)
EMBEDDING_MODEL_NAME = "BAAI/bge-small-zh-v1.5"
EMBEDDING_DIM = 512 # BGE-Small çš„ç»´åº¦
# ç”Ÿæˆæ¨¡å‹è·¯å¾„ (è¯·ç¡®ä¿è·¯å¾„æ­£ç¡®)
BASE_MODEL_PATH = "D:/LLM/Pretrained_models/Qwen/Qwen3-0___6B/" 
LORA_PATH = "./final_law_lora"
CSV_PATH = "./data/law_faq.csv"

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Running on device: {DEVICE}")
# ===============================================================

# --- å…¨å±€å˜é‡ ---
retrieval_model = None
retrieval_tokenizer = None
knowledge_index = None
data_df = None
qwen_model = None
qwen_tokenizer = None

def get_embeddings(sentences):
    """è¾“å…¥æ–‡æœ¬åˆ—è¡¨ï¼Œè¾“å‡ºå½’ä¸€åŒ–çš„å‘é‡"""
    global retrieval_model, retrieval_tokenizer
    
    # æ¨èå¯¹æŸ¥è¯¢ä½¿ç”¨æŒ‡ä»¤å‰ç¼€
    is_query = len(sentences) == 1
    if is_query:
        sentences = [f"ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« : {sentences[0]}"]

    encoded_input = retrieval_tokenizer(
        sentences, 
        padding=True, 
        truncation=True, 
        max_length=512, 
        return_tensors='pt'
    ).to(DEVICE)

    with torch.no_grad():
        model_output = retrieval_model(**encoded_input)
        # BGE ä½¿ç”¨ CLS token (ç¬¬ä¸€ä¸ª token) ä½œä¸ºå¥å‘é‡
        sentence_embeddings = model_output.last_hidden_state[:, 0]
        # å¿…é¡»è¿›è¡Œ L2 å½’ä¸€åŒ– (Normalization)
        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)
        
    return sentence_embeddings.cpu().numpy()

def initialize_models_and_index():
    """åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹å’Œ FAISS ç´¢å¼•"""
    global retrieval_model, retrieval_tokenizer, knowledge_index, data_df, qwen_model, qwen_tokenizer

    # --- 1. åŠ è½½æ£€ç´¢æ¨¡å‹ (BGE-Small) ---
    print(">>> [1/4] æ­£åœ¨åŠ è½½ BGE æ£€ç´¢æ¨¡å‹...")
    # ä½¿ç”¨ SentenceTransformer åº“æ¥ç®€åŒ–åŠ è½½è¿‡ç¨‹
    try:
        model_st = SentenceTransformer(EMBEDDING_MODEL_NAME, device=DEVICE)
        retrieval_model = model_st._first_module() # è·å–åº•å±‚çš„ AutoModel
        retrieval_tokenizer = model_st.tokenizer
    except Exception as e:
        print(f"âŒ åŠ è½½ BGE æ¨¡å‹å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–è·¯å¾„: {e}")
        return False

    # --- 2. æ„å»º FAISS ç´¢å¼• (æ•°æ®åº“å‘é‡åŒ–) ---
    print(">>> [2/4] æ­£åœ¨æ„å»ºå‘é‡åº“...")
    try:
        data_df = pd.read_csv(CSV_PATH)
        data_df = data_df.dropna(subset=["title", "reply"])
        questions = data_df["title"].to_list()
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ° {CSV_PATH}")
        return False

    all_vectors = []
    batch_size = 64
    for i in range(0, len(questions), batch_size):
        batch_sentences = questions[i : i + batch_size]
        batch_emb = get_embeddings(batch_sentences)
        all_vectors.append(batch_emb)
        
    knowledge_vectors = np.concatenate(all_vectors, axis=0)

    knowledge_index = faiss.IndexFlatIP(EMBEDDING_DIM) 
    knowledge_index.add(knowledge_vectors)
    print(f"    âœ… ç´¢å¼•æ„å»ºå®Œæˆï¼åº“å®¹é‡: {knowledge_index.ntotal}, å‘é‡ç»´åº¦: {EMBEDDING_DIM}")

    # --- 3. åŠ è½½ç”Ÿæˆæ¨¡å‹ (Qwen + LoRA) ---
    print(">>> [3/4] æ­£åœ¨åŠ è½½ Qwen + LoRA...")
    try:
        base_model = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL_PATH, 
            device_map="auto", 
            torch_dtype=torch.bfloat16, 
            trust_remote_code=True
        )
        qwen_model = PeftModel.from_pretrained(base_model, LORA_PATH).eval()
        qwen_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
    except Exception as e:
        print(f"âŒ åŠ è½½ Qwen/LoRA æ¨¡å‹å¤±è´¥ï¼Œè¯·æ£€æŸ¥è·¯å¾„: {e}")
        return False

    return True

def rag_chat(user_query, k=3):
    """æ‰§è¡Œ RAG é—®ç­”æµç¨‹"""
    global knowledge_index, data_df, qwen_model, qwen_tokenizer
    
    if knowledge_index is None:
        print("æ¨¡å‹å’Œç´¢å¼•æœªåˆå§‹åŒ–ï¼")
        return ""

    # --- Step A: æ£€ç´¢ ---
    q_vector = get_embeddings([user_query]) 
    scores, indexes = knowledge_index.search(q_vector, k=k)
    
    retrieved_text = []
    print(f"\nğŸ” [æ£€ç´¢è¯¦æƒ…] æŸ¥è¯¢: {user_query}")
    for i, idx in enumerate(indexes[0]):
        if idx != -1 and idx < len(data_df):
            row = data_df.iloc[idx]
            score = scores[0][i]
            print(f"    Rank {i+1}: {row['title']} (Score: {score:.4f})")
            # æ„é€ ä¸Šä¸‹æ–‡æ ¼å¼
            retrieved_text.append(f"ã€æ¡ˆä¾‹{i+1}ã€‘\né—®é¢˜ï¼š{row['title']}\nå›ç­”ï¼š{row['reply']}")
            
    context_str = "\n\n".join(retrieved_text)

    # --- Step B: ç”Ÿæˆ ---
    # æ„é€  Prompt (éµå¾ª Qwen ChatML + RAG)
    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ³•å¾‹æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·å‚è€ƒä¸‹é¢çš„ã€å·²çŸ¥æ¡ˆä¾‹åº“ã€‘ï¼Œå‡†ç¡®å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
å¦‚æœã€å·²çŸ¥æ¡ˆä¾‹åº“ã€‘ä¸­åŒ…å«ç›¸å…³ä¿¡æ¯ï¼Œè¯·ä¼˜å…ˆåŸºäºæ¡ˆä¾‹åº“å›ç­”ã€‚
å¦‚æœæ¡ˆä¾‹åº“ä¸ç›¸å…³ï¼Œè¯·åˆ©ç”¨ä½ çš„ä¸“ä¸šæ³•å¾‹çŸ¥è¯†è¿›è¡Œå›ç­”ã€‚

ã€å·²çŸ¥æ¡ˆä¾‹åº“ã€‘ï¼š
{context_str}"""
    
    prompt = f"<|im_start|>system\n{system_prompt}<|im_end|>\n<|im_start|>user\n{user_query}<|im_end|>\n<|im_start|>assistant\n"
    
    inputs = qwen_tokenizer(prompt, return_tensors="pt").to(qwen_model.device)
    
    with torch.inference_mode():
        outputs = qwen_model.generate(
            **inputs, 
            max_new_tokens=512,
            temperature=0.6, 
            top_p=0.9,
            do_sample=True, # å¯ç”¨é‡‡æ ·
            repetition_penalty=1.1
        )
        
    response = qwen_tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
    return response

if __name__ == "__main__":
    if initialize_models_and_index():
        test_queries = [
            "å¯»è¡…æ»‹äº‹ç½ªä¸€èˆ¬æ€ä¹ˆåˆ¤ï¼Ÿ", 
            "åˆåŒæ³•ä¸­å…³äºè¿çº¦é‡‘çš„è§„å®šæ˜¯ä»€ä¹ˆï¼Ÿ",
            "åŒ—äº¬ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ" # æµ‹è¯•æ— å…³é—®é¢˜ï¼Œçœ‹æ¨¡å‹æ˜¯å¦èƒ½å›ç­”æˆ–æ‹’ç»
        ]
        
        for query in test_queries:
            print("\n" + "="*60)
            print(f"ğŸ‘¤ ç”¨æˆ·é—®é¢˜: {query}")
            final_answer = rag_chat(query)
            print("-"*60)
            print(f"ğŸ¤– AI å›ç­”:\n{final_answer.strip()}")
            print("="*60)
